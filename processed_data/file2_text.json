["Measuring Consistency in Text-based Financial Forecasting Models\nLinyi Yang1,2‚àó, Yingpeng Ma1,2‚àó, Yue Zhang1,2‚Ä†\n1Institute of Advanced Technology, Westlake Institute for Advanced Study\n2School of Engineering, Westlake University\nyanglinyi,mayingpeng,yuezhang@westlake.edu.cn\nAbstract\nFinancial forecasting has been an important\nand active area of machine learning research,\nas even the most modest advantage in predic-\ntive accuracy can be parlayed into significant\nfinancial gains. Recent advances in natural\nlanguage processing (NLP) bring the oppor-\ntunity to leverage textual data, such as earn-\nings reports of publicly traded companies, to\npredict the return rate for an asset. However,\nwhen dealing with such a sensitive task, the\nconsistency of models ‚Äì their invariance un-\nder meaning-preserving alternations in input\n‚Äì is a crucial property for building user trust.\nDespite this, current financial forecasting meth-\nods do not consider consistency. To address\nthis problem, we propose FinTrust, an evalu-\nation tool that assesses logical consistency in\nfinancial text. Using FinTrust, we show that\nthe consistency of state-of-the-art NLP mod-\nels for financial forecasting is poor. Our anal-\nysis of the performance degradation caused\nby meaning-preserving alternations suggests\nthat current text-based methods are not suit-\nable for robustly predicting market informa-\ntion. All resources are available at https:\n//github.com/yingpengma/FinTrust .\n1 Introduction\nNLP techniques have been used in various financial\nforecasting tasks, including stock return prediction,\nvolatility forecasting, portfolio management, and\nmore (Ding et al., 2014, 2015; Qin and Yang, 2019;\nXing et al., 2020; Du and Tanaka-Ishii, 2020; Yang\net al., 2020a; Sawhney et al., 2020). Despite the\nincreased performance of NLP models on financial\napplications, there has been pushback questioning\ntheir trustworthiness, and robustness (Chen et al.,\n2022; Li et al., 2022). Recently, the causal expla-\nnation has been viewed as one of the promising\ndirections for measuring the robustness and thus\n*Equal contribution. Yingpeng Ma did this work during\nhis internship at Westlake University.\nNegation ConsistencyReplace a word with its antonymSymmetric ConsistencyReverse the order of a sentenceAdditive ConsistencyCombine pairs of inputs that shares the same labelTransitive ConsistencyReplace the company name within the same sectorFinTrust\nüéØFigure 1: Examples of four consistency transformations\nused in FinTrust.\nimproving the transparency of models (Stolfo et al.,\n2022; Feder et al., 2022). Among them, consis-\ntency has been viewed as a crucial feature, reflect-\ning the systematic ability to generalize in semanti-\ncally equivalent contexts and receiving increasing\nattention in tasks such as text classification and\nentailment (Jin et al., 2020; Jang et al., 2022).\nPrevious text-based financial forecasting meth-\nods have mostly considered stock movement pre-\ndiction based on various sources of data, including\nfinancial news (Xu and Cohen, 2018; Zhang et al.,\n2018), analyst reports (Kogan et al., 2009; Rekab-\nsaz et al., 2017), and earnings conference calls (Qin\nand Yang, 2019; Keith and Stent, 2019; Li et al.,\n2020; Chen et al., 2021b). While most work evalu-\nates their methods using accuracy and profit gains\nbased on the final outcome in the market (Sawh-\nney et al., 2021b; Yang et al., 2022), consistency\nevaluation remains largely unexplored. The only\nexception (Chuang and Yang, 2022) focuses on\nevaluating the implicit preferences in Pre-trained\nLanguage Models (PLMs) but not the consistency\nin predictive models. The lack of evaluation in be-\nhavior consistency, an important characteristic of\nhuman decisions, hinders the deployment of finan-\ncial forecasting models in real-world scenarios.\nThe main objective of this work is to explore a\nwholistic measure for stock movement prediction,\nintegrating consistency as a criterion of trustworthi-\nness. To this end, we define behavior consistency\nof text-based models in the financial domain. Re-arXiv:2305.08524v2  [cs.CL]  2 Jun 2023", "garding the intrinsic characteristics of financial text\ndata, we consider four types of logical consistency\ntests. As shown in Figure 1, these transformations\ninclude Negation Consistency, Symmetric Consis-\ntency, Additive Consistency, and Transitive Con-\nsistency. Taking negation consistency as an exam-\nple, given an input \"the cost of raw materials has\nbeen greatly decreased\" , if the token \"decreased\"\nis changed to \"increased\" , the model prediction is\nexpected to be flipped accordingly.\nBased on the above logical transformations, we\nintroduce FinTrust, a new evaluation tool that en-\nables researchers to measure consistency in PLMs\nand text-based financial forecasting models. Using\nFinTrust, we design three tasks to investigate the\ninfluence of these logical transformations. First, we\nassess implicit preference in PLMs such as BERT\n(Devlin et al., 2018) and FinBERT (Yang et al.,\n2020b), especially for economic words. Second,\nwe measure the accuracy of stock movement pre-\ndiction on a real-world earnings call dataset after\nthe meaning-preserving modifications. Finally, we\npropose a realistic trading simulation to see if sim-\nple meaning-preserving modifications can wipe out\npositive returns.\nExperiments on several baseline models, includ-\ning previous best-performing architectures (Ding\net al., 2015; Qin and Yang, 2019; Yang et al.,\n2020a) and the machine learning classifier (Chen\net al., 2015) show that all current methods exhibit\na significant decline in the performance of stock\nmovement predictions when evaluating on FinTrust\ncompared to their original results. Notably, some\nmodels demonstrate a level of accuracy that is even\nlower than that of a random guess after undergoing\nlogical consistency transformation, and most meth-\nods fail to surpass the performance of the simplest\nBuy-all strategy in the trading simulation. These\nresults suggest that existing text-based financial\nmodels have robustness and trustworthiness issues,\nwhich can limit their use in practical settings.\nTo our knowledge, FinTrust is the first evalua-\ntion tool for probing if the relatively accurate stock\nmovement prediction is based on the right logi-\ncal behavior. We release our tool and dataset at\nGithub*, which can assist future research in devel-\noping trustworthy FinNLP methods.\n*https://github.com/yingpengma/FinTrust2 Related Work\nText-based Financial Forecasting. A line of work\nhas leveraged event-based neural networks based\non financial news for predicting the stock move-\nment of S&P 500 companies (Ding et al., 2014,\n2015; Xu and Cohen, 2018). By taking advantage\nof recent advances in NLP, recent work has shown\npotential in predicting stock price movements using\nPLMs, BERT (Devlin et al., 2018), and FinBERT\n(Araci, 2019; Yang et al., 2020b), with rich tex-\ntual information from social media and earnings\nconference calls (Liu and Tse, 2013; Xing et al.,\n2020; Chen et al., 2021a). The considerable PLMs\nmainly include BERT and FinBERT. While BERT\nis trained on corpora from fairly general domains,\nFinBERT is trained on financial corpora, includ-\ning earnings conference calls and analyst reports,\nunder the same architecture as BERT. Although\nimplicit stock market preference is in the masked\ntoken predictions task, the implicit preference has\nbeen under-explored using a logical behavior test.\nIn addition to building pre-trained models spe-\ncially trained for financial domains, researchers\nhave recently proposed myriad neural network ar-\nchitectures aimed at more accurate predictions to\nproduce profitable gains including financial risk\n(volatility) and return predictions. For example, re-\nsearchers (Qin and Yang, 2019; Yang et al., 2020a;\nSawhney et al., 2021a) have considered predicting\nthe volatility of publicly traded companies based\non multi-model earnings conference call datasets.\nAlso, Xu and Cohen (2018); Duan et al. (2018);\nYang et al. (2018); Feng et al. (2019) leverage dif-\nferent textual data sources for predicting the stock\nmovement based on the daily closing price. Un-\nfortunately, despite the alarm over the reliance of\nmachine learning systems on spurious patterns that\nhave been found in many classical NLP tasks, the\ntopic of text-based financial forecasting lacks a\nsystematical evaluation regarding the robustness\nanalysis from either an adversarial or consistency\nperspective. To this end, we present the first criti-\ncal investigation of popular benchmarks by using\nFinTrust from the consistency perspective.\nConsistency Measurement. The inductive bias\nof machine learning systems is greatly affected by\nthe patterns in training data due to the nature of\ninductive reasoning. While a flurry of research has\nhighlighted this issue (Gururangan et al., 2018; Sri-\nvastava et al., 2020; Garg and Ramakrishnan, 2020;\nKaushik et al., 2020), recent work Jang et al. (2022)", "Earning Conference Call TranscriptsOriginal Dataset‚Äúthe cost of raw materials has been greatly decreased, with a change of 30% compared with last year‚ÄùEvaluations for Pre-trained ModelsAssessing the implicit preferences in PLMs via masked token predictions‚Äúthe cost of raw materials has been greatly decreased‚Ä¶so the expected return for the next quarter is [MASK]‚ÄùincreasedConsistentdecreasedInconsistentEvaluations for Fine-tuned ModelsNegation ConsistencySymmetric ConsistencyAdditive ConsistencyTransitive ConsistencyFinancial Forecasting Methods(Event CNN; MDRM; HTML)Consistency ScoreFigure 2: Pipelines of FinTrust consist of evaluating pre-trained features (e.g., BERT and FinBERT) and fine-tuned\ntext-based financial forecasting models.\nshows that possible artefacts in data are more influ-\nential than the model design when leading to the\nproblem of lacking trustworthiness. Thus, assess-\ning the influence of data artefacts, such as consis-\ntency, becomes a crucial problem for trustworthy\nNLP. Elazar et al. (2021) study the consistency of\nPLMs (e.g., BERT, ALBERT, and RoBERTa) with\nregard to their knowledge extraction ability and\nconclude that the consistency of these models is\ngenerally low. Chuang and Yang (2022) aim to\nraise awareness of potential implicit stock prefer-\nences based on the finding that consistent implicit\npreference of the stock market exists in PLMs at\nthe whole market.\nIn addition to evaluating preferences in PLMs,\nprevious methods also attempt to evaluate the con-\nsistency of models in downstream NLP tasks, such\nas visual question answering (Ribeiro et al., 2018),\nQA (Jia and Liang, 2017; Ribeiro et al., 2019;\nGan and Ng, 2019; Asai and Hajishirzi, 2020),\nnamed entity recognition (Jia et al., 2019; Wang\nand Henao, 2021), and natural language inference\n(Naik et al., 2018; Hossain et al., 2020; Camburu\net al., 2020; Sinha et al., 2021). Besides, Ribeiro\net al. (2020) consider using consistency for build-\ning the behavioural testing benchmark beyond ac-\ncuracy. Surprisingly, these discussions have not yet\nbeen extended to text-based financial forecasting\nmodels, which require strong robustness to assist\ndecision-making in the financial market, with the\nexception of our work.\n3 Method\nWe define the pipeline of FinTrust in Figure 2. For\ntext-based financial models, there are two salient\ncomponents, namely text representations and fi-\nnancial behavior. For the former, using PLMs has\nbecome a dominant approach, improving the qual-ity of text representations in many domains. For\nthe latter, various neural models can be built on\nPLMs. Correspondingly, we have two setups in\nthe consistency evaluation, representation (Setup\n1) and behavior (Setup 2), respectively.\nSetup 1. In the first stage, we assess the implicit\npreferences in PLMs via masked token predictions.\nIn particular, we first mask a predictable word from\nthe original input extracted from earning confer-\nence call transcripts, such as \"the cost of raw mate-\nrials has been greatly decreased...so the expected\nreturn for the next quarter is [MASK]\" . Then, we\npredict the masked token using PLMs and compare\nthe probability of predicting \"increased\" and \"de-\ncreased\" for contexts from different transcripts. A\nhigher probability of predicting \"increased\" would\nindicate that the given PLM hold logical consis-\ntency with human predictions. Conversely, it sug-\ngests that the prediction of the PLM may be in-\nfluenced by spurious patterns such as favoritism\ntowards a particular stock.\nSetup 2. We evaluate text-based financial fore-\ncasting models after fine-tuning PLMs on a pop-\nularly used earnings conference call dataset (Qin\nand Yang, 2019). However, the consistency mea-\nsurement faces significant challenges in defining\nthe relationship between two texts, particularly\nwhen the text is a long transcript with complex\nlogical connections, such as earnings conference\ncall transcripts. Incorrectly defining this relation-\nship can render consistency judgments meaning-\nless. In line with prior research (Jang et al., 2022),\nwe develop four logical consistency transforma-\ntions customized for financial text in this work. By\nmeaning-preserving altering the original text, we\nensure generated samples have a logical relation-\nship to the original text, thus ensuring the consis-\ntency judgment is meaningful. Below we define", "our text level consistency transformation first (Sec\n3.1), before introducing the financial tasks for be-\nhavior study (Sec 3.2) and a wholistic metric (Sec\n3.3) to integrate performance and trustworthiness.\n3.1 Logical Consistency Transformations on\nText Data\nIn FinTrust, four logical consistency transformation\napproaches are defined to evaluate if the model\nmaintains the same logical behavior as humans,\nrepresenting the consistency in text-based financial\nforecasting models.\nNegation consistency refers to the ability of a\nmodel to generate converse predictions for texts\nwith opposite meanings, i.e. f(x) =positive ‚áî\nf(¬¨x) =negative , where xis the input transcript,\nf(x)represents the output of the model, a \"posi-\ntive\" outcome means the stock price will increase,\nand a \"negative\" outcome means the stock price\nwill decrease. ¬¨xis a negation consistency trans-\nformed test example flipped through predetermined\nrules based on the bi-grams of the most frequent\nwords and their antonyms. We achieve this by split-\nting the dataset at the sentence level and flipping\nthe meanings of sentences. Given an input \"the\ncost of raw materials has been greatly decreased,\nwith a change of 30% compared with last year\" ,\nits counterpart can be \"the cost of raw materials\nhas been greatly increased, with a change of 30%\ncompared with last year\" . In the financial market,\na significant cost reduction may lead to optimism\nabout the company‚Äôs future prospects and an in-\ncrease in stock price. Only when the model can\ngive the correct predictions for both pairs of test-\ning data we consider that the model is consistent\nwith non-contradictory predictions. Otherwise, it\nis considered to lack negation consistency.\nSymmetric consistency is the property of a\nmodel where the order of the inputs does not af-\nfect the output. It is defined as f(Sp1, Sp2) =\nf(Sp2, Sp1), where Sis a sentence in the transcript,\nSpirepresents the part iof the sentence. This can\nbe tested by reordering the segments of each sen-\ntence in the transcript and comparing the predic-\ntions before and after the reordering. For example,\ngiven the sentence \"the cost of raw materials has\nbeen greatly decreased, with a change of 30% com-\npared with last year\" , if the prediction is reversed\nafter reordering it to \"with a change of 30% com-\npared with last year, the cost of raw materials has\nbeen greatly decreased\" , then the model is regardedas lacking symmetric consistency.\nAdditive consistency refers to the property of\na model to predict the stock movement based on\nthe combination of two inputs, xandythat share\nthe same label. The model is expected to hold the\nsame prediction for x,y, and the concatenation of\nthose inputs x+y. If the model produces different\npredictions for the above three kinds of inputs, it\ncan be regarded as lacking additive consistency.\nFor example, if a model gives a positive prediction\nfor the sentence \"the cost of raw materials has been\ngreatly decreased, with a change of 30% compared\nwith last year\" , and also gives a positive prediction\nfor the sentence \"we believe that our products can\nbring convenience to everyone‚Äôs life\" , then it should\nalso make a positive prediction for the combined\nsentences after the concatenation.\nTransitive consistency refers to the ability of\na model where the perceived sentiment of a com-\npany should be reflected in the performance of the\ntop-valued company in the same industry. It can\nbe expressed as f(x) =f(x‚Ä≤), where x‚Ä≤represents\ntransitive consistency transformed text. Specifi-\ncally, for transcripts of a particular company, the\ntop-valued company in the same industry is identi-\nfied and its name is denoted as \"company_name\".\nThen occurrences of words such as \"we\" and \"our\"\nare replaced with \"company_name\" and \"com-\npany_name‚Äôs\" respectively. For example, if the\ncorresponding sector of the company is \"Informa-\ntion Technology\" and the top-valued company in\nthe S&P 500 is Apple Inc., a sentence such as\n\"we believe that our products can bring conve-\nnience to everyone‚Äôs life \" will be transformed to\n\"Apple Inc. believe that Apple Inc. ‚Äôs products can\nbring convenience to everyone‚Äôs life \" after transi-\ntive consistency transformation. Again, we cal-\nculate the consistency of models by considering\nthe non-contradictory predictions over transitive\ninstances.\n3.2 Prediction Tasks in FinTrust\nConsistency Measurement in PLMs. To better\nassess the implicit preference in PLMs, we extend\nthe previous cloze-style prompts used in assess-\ning stock market preference (Chuang and Yang,\n2022) by considering logical changes rather than\nsimply predicting the masked token in the input.\nThis is crucial as if PLMs are biased, the fine-tuned\nmodel‚Äôs predictions based on features learned by\nPLMs could be further influenced by spurious pref-", "erence tendencies, which would negatively impact\nthe effect of financial forecasting.\nStock Prediction Task. Following previous stud-\nies (Ding et al., 2015; Duan et al., 2018; Sawhney\net al., 2020), we treat the stock movement predic-\ntion as a binary classification problem, where the\nmodel predicts whether the daily closing price of a\ngiven asset will increase or decrease over the next n\ndays ( n=3, 7, 15, 30) based on the content of earn-\nings call transcripts. The output is either ‚Äúincrease‚Äù\n(positive) or ‚Äúdecrease‚Äù (negative).\nTrading Simulation Task. We use the predictions\nto determine whether to buy or sell a stock after n\ndays. For example, if the model predicts that the\nstock price would increase from day dto day d+30,\nwe would buy the stock on day dand sell it on day\nd+ 30 . Otherwise, we execute a short sell. The\nprevious work Sawhney et al. (2021a) simulates the\ntrade of one hand for each stock, which allows for\nthe potential offset of multiple forecast failures if\none stock is more valuable. However, this approach\nis unfair under specific situations since each predic-\ntion and trade are treated equally and thus will lose\nthe balance between trades. Therefore, we invest\nthe same amount of money in each stock and calcu-\nlate the profit ratio instead of the cumulative profit.\nThis method does not affect the calculation of the\nSharpe Ratio and allows us to explore the impact of\nfinancial forecasting consistency on performance\nand profitability. Notably, we do not consider the\ntransaction cost in accordance with previous work\n(Sawhney et al., 2021a).\n3.3 Wholistic Evaluation Metrics\nWe introduce the predictive evaluation metrics and\nthe novel consistency evaluation metrics as elabo-\nrated below.\nPredictive Evaluations. For stock prediction, we\nuse three metrics to measure performance: Accu-\nracy, F1 score, and Matthews correlation coefficient\n(MCC). These metrics are calculated as follows:\nF1 =2√óprecision √órecall\nprecision +recall(1)\nFor a given confusion matrix:\nAccuracy =tp+tn\ntp+tn+fp+fn(2)\nMCC =tp√ótn‚àífp√ófnp\n(tp+fp)(tp+fn)(tn+fp)(tn+fn)(3)\nWe use both Profit Ratio and Sharpe Ratio for the\ntrading simulation task as performance indicators.Return R, and investment Iis involved in calculat-\ning the Profit Ratio.\nProfitRatio =R\nI(4)\nThe Sharpe Ratio measures the performance of an\ninvestment by considering the average return Rx,\nrisk-free return Rf, and standard deviation of the\ninvestment œÉ(Rx).\nSharpeRatio =Rx‚àíRf\nœÉ(Rx)(5)\nConsistence Evaluations. Based on logical trans-\nformations, we propose the consistency evaluation\nmetrics of consistency, aiming to measure text-\nbased financial forecasting models from a consis-\ntency perspective as a complementary metric to\naccuracy. Assuming that Cis a set of four log-\nical consistencies. To begin with, we define the\nconsistency score ( Consis ), elaborated as follows:\nConsis =P|C|\ni=1Ci\n|C|(6)\nwhere the Cset contains Negation consistency\nConsisN, Symmetric consistency ConsisS, Addi-\ntive consistency ConsisA, Transitive consistency\nConsisT. We give the formal definition of those\nfour metrics, respectively. The consistency of\nConsisNis calculated as:\nConsisN=P|D|\ni=1\u001a0 (f(xi) =f(xN\ni))\n1 (f(xi)Ã∏=f(xN\ni))\n|D|(7)\nwhere Dis the original test set, xiis the test sample\nin the original test set, i.e. xi‚ààD.xN\niis the\nnew test sample obtained by negation consistency\ntransformation on xi, and f(x)is the prediction\nof the model (positive or negative) for the input x.\nIn terms of the symmetric, additive, and transitive\ntransformations, the value equals 0 when f(xi)Ã∏=\nf(xN\ni)while equals 1 when f(xi) =f(xN\ni).\n4 Experiments\nWe first evaluate the explicit preferences in PLMs.\nThen we assess the ability of text-based models to\nmake consistent predictions on the stock movement\nand finally test the profitability of these predictions\nusing a trading simulation.", "4.1 Dataset\nEarnings Call Data. We use the publicly avail-\nable Earning Conference Calls dataset by (Qin\nand Yang, 2019), which includes transcripts of 576\nearnings calls from S&P 500 companies listed on\nthe American Stock Exchange, obtained from the\nSeeking Alpha website. It also includes the meta-\ninformation on the company affiliations and publi-\ncation dates.\nFinancial Market information. We also collect\nhistorical price data (closing price) for the traded\ncompanies listed in S&P 500 from Yahoo Finance\nfor the period from January 1, 2017, to January 31,\n2018. This data was used to calculate the label of\nstock price movement and profitability.\nData Processing. Following (Qin and Yang, 2019;\nYang et al., 2020a), we split the dataset into mu-\ntually exclusive train/validation/test sets in a 7:1:2\nratio in chronological order to ensure that future\ninformation is not used to predict past price move-\nments. We also construct logical consistency\ndatasets based on the original test set using the\nabove-mentioned four logical consistency transfor-\nmations. The size of our evaluation dataset is four\ntimes the size of the original one since we ensure\nthat each sample in the original test set corresponds\nto four logical consistency test samples. To facili-\ntate future research, we release our dataset and the\nevaluation toolkit in FinTrust .\n4.2 Models\nRepresentation Models. We conduct experiments\non popular PLMs, including BERT (Devlin et al.,\n2018), RoBERTa (Liu et al., 2019), DistilBERT\n(Sanh et al., 2019), and FinBERT (Yang et al.,\n2020b). The vocabulary of FinBERT is different\nfrom the others as it contains domain-specific terms\nin the financial market, including company names.\nPredictive Models. Regarding the forecasting\nmodels, we evaluate several baselines, including\nthe traditional machine learning and state-of-the-art\ntransformer-based methods, detailed as follows.\n‚Ä¢HTML: Yang et al. (2020a) propose a hier-\narchical transformer-based framework to ad-\ndress the problem of processing long texts in\nearnings call data. It utilizes a pre-trained\nWWM-BERT-Large model to generate sen-\ntence representations as inputs for the model.\n‚Ä¢MRDM: Qin and Yang (2019) propose the\nfirst method to treat volatility prediction as aPLM Params Neg Pos Consistency\nBERT-base 110M + + 71.33%\nBERT-base 110M + - 55.87%\nBERT-base 110M - + 86.79%\nBERT-large 340M + + 75.67%\nBERT-large 340M + - 67.60%\nBERT-large 340M - + 83.74%\nRoBERTa-base 125M + + 77.79%\nRoBERTa-base 125M + - 69.17%\nRoBERTa-base 125M - + 86.40%\nRoBERTa-large 355M + + 82.70%\nRoBERTa-large 355M + - 76.67%\nRoBERTa-large 355M - + 88.72%\nFinBERT 110M + + 72.40%\nFinBERT 110M + - 56.27%\nFinBERT 110M - + 88.53%\nDistilBERT 66M + + 70.13%\nDistilBERT 66M + - 57.92%\nDistilBERT 66M - + 82.33%\nTable 1: The results of the consistency measurement in\nPLMs via masked token predictions, splitting by neg-\native and positive token predictions. ‚Äò+‚Äô denotes that\nthe attitude of the word with the specific polarity will\nbe predicted while ‚Äò-‚Äô means that we do not consider\ntokens with a specific polarity.\nmulti-modal deep regression problem, build-\ning benchmark results and introducing the\nearnings conference call dataset.\n‚Ä¢Event: Ding et al. (2015) adapt Open IE for\nevent-based stock price movement prediction,\nextracting structured events from large-scale\npublic news without manual efforts.\n‚Ä¢XGBoost: Chen et al. (2015) propose a\ngradient-boosting decision tree known as the\nclassical machine learning baseline.\n5 Results and Discussion\nWe report the results of three tasks defined in Sec-\ntion 3.2 and the consistency score calculated by the\nconsistency evaluation metrics in this section. Fur-\nthermore, we present extensive ablation studies and\ndiscussions to support in-depth analyses of each\ncomponent in FinTrust.\n5.1 Predictive Results\nConsistency Measurement in PLMs. The results\nof explicit preferences in PLMs are presented in\nTable 1. In general, we find that all PLMs ex-\nhibited relatively low consistency, ranging from\n70.13% to 82.7%, which falls significantly short\nof the level of robustness expected in the finan-\ncial market. Also, we observe that PLMs typically\ndemonstrated lower consistency when tested on", "Metrics ACC F1 MCC\nPeriod Avg 3 7 15 30 Avg 3 7 15 30 Avg 3 7 15 30\nHTML 0.546 0.442 0.531 0.566 0.646 0.671 0.571 0.619 0.713 0.780 0.078 0.052 0.056 0.032 0.175\n+FinTrust 0.521‚Üì0.465‚Üë0.527‚Üì0.529‚Üì0.564‚Üì0.647‚Üì0.608‚Üë0.629‚Üë0.648‚Üì0.703‚Üì0.040‚Üì 0.019‚Üì 0.058‚Üë 0.019‚Üì0.063‚Üì\nMRDM 0.555 0.504 0.513 0.584 0.619 0.670 0.541 0.663 0.722 0.754 0.059 0.079 0.007 0.107 0.044\n+FinTrust 0.504‚Üì0.465‚Üì0.511‚Üì0.507‚Üì0.535‚Üì0.622‚Üì0.569‚Üë0.667‚Üë0.578‚Üì0.674‚Üì0.017‚Üì -0.024‚Üì0.038‚Üë 0.032‚Üì0.023‚Üì\nEvent 0.542 0.416 0.522 0.593 0.637 0.694 0.582 0.682 0.736 0.776 0.122 0.078 0.097 0.189 0.123\n+FinTrust 0.512‚Üì0.447‚Üë0.504‚Üì0.529‚Üì0.569‚Üì0.656‚Üì0.598‚Üë0.663‚Üì0.658‚Üì0.705‚Üì0.006‚Üì -0.032‚Üì-0.023‚Üì0.013‚Üì0.068‚Üì\nXGB 0.515 0.434 0.487 0.584 0.558 0.561 0.448 0.500 0.641 0.653 0.018 -0.093 -0.027 0.147 0.043\n+FinTrust 0.507‚Üì0.462‚Üë0.502‚Üë0.531‚Üì0.531‚Üì0.545‚Üì0.456‚Üë0.518‚Üë0.584‚Üì0.622‚Üì-0.004‚Üì-0.076‚Üë-0.002‚Üë0.045‚Üì0.014‚Üì\nTable 2: Performance and robustness evaluation of stock movement prediction for multiple baselines using FinTrust.\nSignificant performance decay has been observed on all methods using the Student T-test over 10 times run, p <0.05.\nnegative tokens than positive tokens (on average\n63.91% ‚Äì negative vs. 86.09% ‚Äì positive). This\nsuggests that popular PLMs tend to exhibit stereo-\ntypes when predicting negative tokens.\nFrom a model-level perspective, our results in-\ndicate that FinBERT, which utilizes a domain-\nspecific training corpus during the pre-training\nphase, can slightly improve consistency compared\nto BERT-base. Besides, we show that the increase\nin parameter size brings significant benefits for im-\nproving consistency, given that BERT-large and\nRoBERTa-large both outperform their base-sized\nversions (75.67% vs. 71.33% ‚Äì BERT; 82.70%\nvs.77.79% ‚ÄìRoBERTa). In particular, RoBERTa-\nachieves the highest consistency across three set-\ntings, indicating its high robustness. In contrast,\nDistilBERT achieves the lowest consistency.\nStock Movement Prediction. The results of stock\nmovement prediction over text-based financial fore-\ncasting models are shown in Table 2. We evaluate\nmultiple baselines by comparing the results of mod-\nels on the original test set to the results tested on\ntransformed datasets (shown as +FinTrust). It is\nnoteworthy that the accuracy of some predictions\nis even lower than that of random guess, especially\nfor the short-time prediction (n=3). Furthermore,\nwe demonstrate that the effect of logical consis-\ntency transformations on traditional performance\nindicators varies depending on the time period, but\nthe average performance of all models decreased\nsignificantly over three metrics. In particular, mod-\nels show extraordinary vulnerability when it comes\nto predicting the long-term stock return (n=15 and\n30), as transformations in all settings decrease ac-\ncuracy when the time period is 15 and 30 days.\nFrom the model perspective, regarding the ratio\nof performance decay, XGBoost is the least im-\npacted, and MRDM is the most affected. This can\nbe because traditional machine learning models,\nsuch as XGBoost, have fewer parameters than deep\nlearning models and are therefore less affected byStrategy Profit Ratio Sharpe Ratio\nHTML 3.752 0.266\n+ FinTrust 3.359 ‚Üì 0.229‚Üì\n‚àÜ‚Üì -10% -14%\nEvent 3.720 0.263\n+ FinTrust 3.535 ‚Üì 0.245‚Üì\n‚àÜ‚Üì -5% -7%\nMRDM 3.495 0.241\n+ FinTrust 2.384 ‚Üì 0.138‚Üì\n‚àÜ‚Üì -32% -43%\nXGB -0.515 -0.126\n+ FinTrust 0.296 ‚Üë 0.032‚Üë\n‚àÜ‚Üë 158% 75%\nBuy-all 3.681 0.259\nRandom -0.271 -0.105\nShort-sell-all -3.681 -0.259\nTable 3: Performance on the trading simulation. ‚Äò+Fin-\nTrust‚Äô represents the performance using the input after\nthe transformation.\nartefacts. Despite this, the accuracy on FinTrust\nachieved by models is only slightly more accu-\nrate than the random guess (e.g., 0.504 on MRDM,\n0.507 on XGBoost). The vulnerability of these\nmodels, including state-of-the-art methods, hinders\nthe deployment of NLP systems in the real financial\nmarket and should be taken more seriously.\nTrading Simulation. We compare three simple\ntrading strategies (Buy-all, Short-sell-all, and Ran-\ndom) with four baselines. The results are shown\nin Table 3. It can be seen that HTML and Event\nhave higher yields and can exceed simple trading\nstrategies. However, after conducting consistency\ntransformations, positive returns of these two meth-\nods are much reduced, even lower than the simple\nBuy-all strategy. Methods such as MRDM and XG-\nBoost gain lower returns than Buy-all, with MRDM\nexperiencing the highest drop of about 32%-43% .\nEven though the returns of XGBoost improved sig-\nnificantly after the transformations, it still remained\nmuch lower than the Buy-all strategy and the other\nthree baselines. Hence, we contend that the in-\ncrease in XGBoost‚Äôs returns does not have a strong\nreference value. We conclude that most methods", "Period 3 7 15 30\nA VG 0.730 0.739 0.644 0.692\nAdd 0.903 0.947 0.664 0.805\nEvent Neg 0.106 0.035 0.044 0.018\nSym 0.947 0.982 0.929 0.973\nTra 0.965 0.991 0.938 0.973\nA VG 0.699 0.628 0.688 0.684\nAdd 0.894 0.655 0.876 0.743\nHTML Neg 0.115 0.212 0.177 0.009\nSym 0.894 0.796 0.841 0.991\nTra 0.894 0.850 0.858 0.991\nA VG 0.597 0.706 0.524 0.650\nAdd 0.664 0.894 0.301 0.735\nMRDM Neg 0.248 0.062 0.053 0.053\nSym 0.655 0.894 0.805 0.885\nTra 0.823 0.973 0.938 0.929\nA VG 0.566 0.595 0.593 0.653\nAdd 0.522 0.487 0.496 0.504\nXGB Neg 0.071 0.133 0.124 0.354\nSym 1.000 0.973 0.973 0.991\nTra 0.673 0.788 0.779 0.761\nTable 4: The consistency score calculated by Consis .\nshow unacceptably poor performance caused by\nlacking consistent logical behavior.\n5.2 Consistency Score\nResults. We show the results of the consistency\nscore (defined in Section 3.4) in Table 4. It can be\nseen that Event has the highest consistency score\n(Consis ) and XGBoost has the lowest Consis . Re-\ngarding the average consistency over four transfor-\nmations, Event achieves three of the four highest\nconsistency scores. XGBoost tends to make con-\ntradictory predictions in terms of the lowest scores\nin three settings. Additionally, all methods per-\nform poorly on negation consistency, consistent\nwith findings in the PLMs evaluation (Table 1).\nCorrelation Analysis. We examine the correlation\nbetween the indicators of consistency and accu-\nracy. Importantly, we find that our consistency\nscore does not align with traditional performance\nindicators such as accuracy, evidenced by the fact\nthat the most consistent model (Event) is not neces-\nsarily the highest in accuracy (HTML). The overall\nPearson correlation coefficient between the consis-\ntency score and accuracy is only 0.314, indicating\na low-level correlation. This suggests that the pro-\nposed consistency score can be used as a comple-\nmentary evaluation metric for accuracy in future\nresearch on text-based financial forecasting.\n5.3 Discussion\nHuman Evaluation. To assess the effectiveness of\nour consistency transformation method in preserv-\ning the original meaning, we conduct a human an-\n/uni00000032/uni00000035/uni0000002c /uni00000024/uni00000039/uni0000002a /uni00000024/uni00000027/uni00000027 /uni00000031/uni00000028/uni0000002a /uni00000036/uni0000003c/uni00000030 /uni00000037/uni00000035/uni00000024/uni00000013/uni00000011/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013\n/uni0000002b/uni00000037/uni00000030/uni0000002f\n/uni00000030/uni00000035/uni00000027/uni00000030\n/uni00000028/uni00000059/uni00000048/uni00000051/uni00000057\n/uni0000003b/uni0000002a/uni00000025Figure 3: The ablation study of FinTrust in stock move-\nment prediction on the average MCC over periods.\nnotation study. Two annotators are employed from\nthe author list and be required to label each sam-\nple and its four consistency transformations. Both\nof them received an advanced degree in computer\nscience. The Inter-Annotator Agreement score is\ncalculated to be 0.98, based on an evaluation of 40\nsamples and their 160 transformed samples. The\naverage consistency score for human annotators\nis 0.975, indicating that our method successfully\npreserves the original meaning in most cases.\nAblation Study. We show the ablation results\nin stock movement prediction of four transforma-\ntions in Figure 3. We find that evaluations on the\nFinTrust lead to significant performance decay for\nmost settings compared to the original performance,\nwhich illustrates the individual influence of trans-\nformations. In particular, we show that models\nusually underperform when evaluating the nega-\ntion transformation , with the exception of MRDM.\nIt suggests that current models lack the ability to\nprovide non-contradictory predictions.\n6 Conclusion\nWe proposed FinTrust, an evaluation tool that as-\nsesses the trustworthiness of financial forecasting\nmodels in addition to their accuracy. Results on\nFinTrust show that (1) the consistency of state-of-\nthe-art models falls significantly short of expecta-\ntions when applied to stock movement prediction;\n(2) predictions with such a low logical consistency\ncan lead to severe consequences, as evidenced by\npoor performance in a trading simulation test. Our\nempirical results highlight the importance of per-\nceiving such concerns when developing and evalu-\nating text-based financial models, and we release\nour dataset for facilitating future research. Despite\nthis, how to evaluate the consistency of large-scale\nlanguage models (LLMs) is still an open question", "towards the financial forecasting task.\nLimitation\nWhile our pipeline is designed to be applicable to\nany financial text dataset, the evaluation dataset is\ntransformed solely on earnings conference calls.\nWe will expand the scope of experiments to include\nother financial text sources such as news articles\nand social media posts. Finally, the current trad-\ning simulation does not take transaction costs into\naccount. Going forward it will be necessary to\nconsider more sophisticated trading policies.\nEthics Statement\nThis paper honors the ACL Code of Ethics. The\ndataset used in the paper does not contain any pri-\nvate information. All annotators have received\nenough labor fees corresponding to their amount of\nannotated instances. The code and data are open-\nsourced under the CC-BY-NC-SA license.\nAcknowledgements\nWe would like to thank anonymous reviewers\nfor their insightful comments and suggestions to\nhelp improve the paper. This publication has em-\nanated from research conducted with the finan-\ncial support of the Pioneer and ‚ÄúLeading Goose\"\nR&D Program of Zhejiang under Grant Num-\nber 2022SDXHDX0003, the 72nd round of the\nChinese Post-doctoral Science Foundation project\n2022M722836, and the financial support from the\nrxhui.com company. Yue Zhang is the correspond-\ning author.\nReferences\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models. arXiv preprint\narXiv:1908.10063 .\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-\nguided data augmentation and regularization for con-\nsistent question answering. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics , pages 5642‚Äì5650.\nOana-Maria Camburu, Brendan Shillingford, Pasquale\nMinervini, Thomas Lukasiewicz, and Phil Blunsom.\n2020. Make up your mind! adversarial generation\nof inconsistent natural language explanations. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4157‚Äì\n4165.Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen.\n2021a. Evaluating the rationales of amateur investors.\nInThe World Wide Web Conference .\nChung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen.\n2021b. From opinion mining to financial argument\nmining. Springer Briefs in Computer Science , pages\n1‚Äì95.\nChung-Chi Chen, Hiroya Takamura, and Hsin-Hsi\nChen. 2022. Fintech for social good: A re-\nsearch agenda from nlp perspective. arXiv preprint\narXiv:2211.06431 .\nTianqi Chen, Tong He, Michael Benesty, Vadim\nKhotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen,\net al. 2015. Xgboost: extreme gradient boosting. R\npackage version 0.4-2 , 1(4):1‚Äì4.\nChengyu Chuang and Yi Yang. 2022. Buy tesla, sell\nford: Assessing implicit stock market preference in\npre-trained language models. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers) , pages\n100‚Äì105, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nXiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.\n2014. Using structured events to predict stock price\nmovement: An empirical investigation. In Proceed-\nings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1415‚Äì1425.\nXiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.\n2015. Deep learning for event-driven stock predic-\ntion. In Proceedings of the 24th International Con-\nference on Artificial Intelligence , page 2327‚Äì2333,\nBuenos Aires, Argentina.\nXin Du and Kumiko Tanaka-Ishii. 2020. Stock embed-\ndings acquired from news articles and price history,\nand an application to portfolio optimization. In Pro-\nceedings of the 58th annual meeting of the associa-\ntion for computational linguistics , pages 3353‚Äì3363.\nJunwen Duan, Yue Zhang, Xiao Ding, Ching Yun\nChang, and Ting Liu. 2018. Learning target-specific\nrepresentations of financial news documents for cu-\nmulative abnormal return prediction. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics (COLING-18) , pages 2823‚Äì2833.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Sch√ºtze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012‚Äì1031.", "Amir Feder, Katherine A Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob\nEisenstein, Justin Grimmer, Roi Reichart, Margaret E\nRoberts, et al. 2022. Causal inference in natural lan-\nguage processing: Estimation, prediction, interpreta-\ntion and beyond. Transactions of the Association for\nComputational Linguistics , 10:1138‚Äì1158.\nFuli Feng, Huimin Chen, Xiangnan He, Ji Ding,\nMaosong Sun, and Tat-Seng Chua. 2019. Enhancing\nstock movement prediction with adversarial training.\narXiv preprint arXiv:1810.09936 .\nWee Chung Gan and Hwee Tou Ng. 2019. Improv-\ning the robustness of question answering systems to\nquestion paraphrasing. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 6065‚Äì6075.\nSiddhant Garg and Goutham Ramakrishnan. 2020. Bae:\nBert-based adversarial examples for text classifica-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP) , pages 6174‚Äì6181.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel R Bowman, and Noah A\nSmith. 2018. Annotation artifacts in natural language\ninference data. arXiv preprint arXiv:1803.02324 .\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020. An analysis of natural language infer-\nence benchmarks through the lens of negation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 9106‚Äì9118, Online. Association for Computa-\ntional Linguistics.\nMyeongjun Jang, Deuk Sin Kwon, and Thomas\nLukasiewicz. 2022. Becel: Benchmark for consis-\ntency evaluation of language models. In Proceedings\nof the 29th International Conference on Computa-\ntional Linguistics , pages 3680‚Äì3696.\nChen Jia, Xiaobo Liang, and Yue Zhang. 2019. Cross-\ndomain NER using cross-domain language modeling.\nInProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics , pages 2464‚Äì\n2474, Florence, Italy. Association for Computational\nLinguistics.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nInProceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2021‚Äì2031.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classification\nand entailment. In Proceedings of the AAAI con-\nference on artificial intelligence , volume 34, pages\n8018‚Äì8025.Divyansh Kaushik, Eduard Hovy, and Zachary Lipton.\n2020. Learning the difference that makes a differ-\nence with counterfactually-augmented data. In Inter-\nnational Conference on Learning Representations .\nKatherine Keith and Amanda Stent. 2019. Modeling\nfinancial analysts‚Äô decision making via the pragmat-\nics and semantics of earnings calls. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , ACL ‚Äô19, pages 493‚Äì503,\nFlorence, Italy.\nShimon Kogan, Dimitry Levin, Bryan R. Routledge,\nJacob S. Sagi, and Noah A. Smith. 2009. Pre-\ndicting risk from financial reports with regression.\nInProceedings of Human Language Technologies:\nThe 2009 Annual Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics , pages 272‚Äì280.\nHao Li, Jie Shao, Kewen Liao, and Mingjian Tang.\n2022. Do simpler statistical methods perform better\nin multivariate long sequence time-series forecasting?\nInProceedings of the 31st ACM International Con-\nference on Information & Knowledge Management ,\npages 4168‚Äì4172.\nJiazheng Li, Linyi Yang, Barry Smyth, and Ruihai Dong.\n2020. Maec: A multimodal aligned earnings confer-\nence call dataset for financial risk prediction. In Pro-\nceedings of the 29th ACM International Conference\non Information & Knowledge Management , pages\n3063‚Äì3070.\nShouwei Liu and Yiu Kuen Tse. 2013. Estimation of\nmonthly volatility: An empirical comparison of real-\nized volatility, garch and acd-icv methods. Finance\nResearch Letters .\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress test evaluation for natural language inference.\nInProceedings of the 27th International Conference\non Computational Linguistics , pages 2340‚Äì2353,\nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics.\nYu Qin and Yi Yang. 2019. What you say and how you\nsay it matters: Predicting stock volatility using verbal\nand vocal cues. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics , pages 390‚Äì401, Florence, Italy. Association\nfor Computational Linguistics.\nNavid Rekabsaz, Mihai Lupu, Artem Baklanov, Alexan-\nder D√ºr, Linda Andersson, and Allan Hanbury. 2017.\nV olatility prediction using financial disclosures sen-\ntiments with word embedding-based ir models. In", "Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 1712‚Äì1721.\nMarco Tulio Ribeiro, Carlos Guestrin, and Sameer\nSingh. 2019. Are red roses red? evaluating consis-\ntency of question-answering models. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 6174‚Äì6184.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversar-\nial rules for debugging nlp models. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 856‚Äì865.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4902‚Äì\n4912, Online. Association for Computational Lin-\nguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 .\nRamit Sawhney, Arshiya Aggarwal, and Rajiv Shah.\n2021a. An empirical investigation of bias in the\nmultimodal analysis of financial earnings calls. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 3751‚Äì3757.\nRamit Sawhney, Mihir Goyal, Prakhar Goel, Puneet\nMathur, and Rajiv Ratn Shah. 2021b. Multimodal\nmulti-speaker merger & acquisition financial mod-\neling: A new task, dataset, and neural baselines. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 6751‚Äì\n6762, Online. Association for Computational Lin-\nguistics.\nRamit Sawhney, Puneet Mathur, Ayush Mangal, Piyush\nKhanna, Rajiv Ratn Shah, and Roger Zimmermann.\n2020. Multimodal multi-task financial risk forecast-\ning. In Proceedings of the 28th ACM International\nConference on Multimedia , MM ‚Äô20, page 456‚Äì465.\nAssociation for Computing Machinery.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,\nand Adina Williams. 2021. UnNatural Language\nInference. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers) ,\npages 7329‚Äì7346, Online. Association for Computa-\ntional Linguistics.Megha Srivastava, Tatsunori Hashimoto, and Percy\nLiang. 2020. Robustness to spurious correlations\nvia human annotations. In International Conference\non Machine Learning , pages 9109‚Äì9119. PMLR.\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Sch√∂lkopf, and Mrinmaya Sachan. 2022. A\ncausal framework to quantify the robustness of math-\nematical reasoning with language models. arXiv\npreprint arXiv:2210.12023 .\nRui Wang and Ricardo Henao. 2021. Unsupervised\nparaphrasing consistency training for low resource\nnamed entity recognition. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 5303‚Äì5308, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nFrank Xing, Lorenzo Malandri, Yue Zhang, and Erik\nCambria. 2020. Financial sentiment analysis: an\ninvestigation into common mistakes and silver bullets.\nInProceedings of the 28th international conference\non computational linguistics , pages 978‚Äì987.\nYumo Xu and Shay B Cohen. 2018. Stock movement\nprediction from tweets and historical prices. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers) , pages 1970‚Äì1979.\nLinyi Yang, Jiazheng Li, Ruihai Dong, Yue Zhang, and\nBarry Smyth. 2022. Numhtml: Numeric-oriented hi-\nerarchical transformer model for multi-task financial\nforecasting. In AAAI .\nLinyi Yang, Tin Lok James Ng, Barry Smyth, and Ri-\nuhai Dong. 2020a. Html: Hierarchical transformer-\nbased multi-task learning for volatility prediction.\nInProceedings of The Web Conference 2020 , pages\n441‚Äì451.\nLinyi Yang, Zheng Zhang, Su Xiong, Lirui Wei, James\nNg, Lina Xu, and Ruihai Dong. 2018. Explainable\ntext-driven neural network for stock prediction. In\n2018 5th IEEE International Conference on Cloud\nComputing and Intelligence Systems (CCIS) , pages\n441‚Äì445. IEEE.\nYi Yang, Mark Christopher Siy UY , and Allen Huang.\n2020b. Finbert: A pretrained language model\nfor financial communications. arXiv preprint\narXiv:2006.08097 .\nXi Zhang, Yunjia Zhang, Senzhang Wang, Yuntao Yao,\nBinxing Fang, and S Yu Philip. 2018. Improving\nstock market prediction via heterogeneous informa-\ntion fusion. Knowledge-Based Systems , 143:236‚Äì\n247.", "A Transitive Consistency\nExample. We show an example to understand bet-\nter the motivation for using Transitive Consistency\nwhen measuring the consistency of FinNLP mod-\nels. Given \"Nektar Therapeutics gave investors\nstrong confidence after Earnings Conference Call\non March 1, 2017, and its stock price soared\n79.43% in the following month.\" . As a leading\ncompany in the same Sector (Health Care), John-\nson & Johnson (JNJ) was also affected by this and\nincreased by 1.91% over the same period, which\nconfirmed the rationality of selecting transitive con-\nsistency as one of the measurement methods.\nB Full Ablation Results\nWe report the ablation study results of four differ-\nent types of logical transformation based on the\nfine-tuned forecasting models in Table 5. We use\nitalics to indicate the performance before consis-\ntency transformation, use bold to express the per-\nformance that has been reduced after consistency\ntransformation, and do not deal with other parts that\nhave not decreased, for the convenience of readers.\nAll detailed return changes in trading simulation\nbased on text-based fine-tuned forecasting models\nare also shown in Table 6. \"+FinTrust \" means the\naverage impact of the four transformations.\nC Additional experimental details\nThe model settings involved in the paper are all\naligned with the parameters and training details\ndescribed in the corresponding article Yang et al.\n(2020a); Qin and Yang (2019); Ding et al. (2015);\nChen et al. (2015). The total computational budget\nis about 50 GPU hours, using a GeForce RTX 3090.\nAll models use the highest performance among\nten repeated experiments using different seeds and\nensure reproducibility.", "ACC F1 MCC\n3 7 15 30 Avg 3 7 15 30 Avg 3 7 15 30 Avg\nHTML 0.442 0.531 0.566 0.646 0.546 0.571 0.619 0.713 0.780 0.671 0.052 0.056 0.032 0.175 0.078\nHTML-Add 0.407 0.540 0.584 0.619 0.538 0.579 0.662 0.715 0.726 0.671 0.000 0.085 0.104 0.127 0.079\nHTML-Neg 0.602 0.522 0.416 0.363 0.476 0.737 0.625 0.522 0.532 0.604 0.089 0.073 -0.113 -0.123 -0.019\nHTML-Sym 0.425 0.522 0.566 0.637 0.538 0.564 0.620 0.684 0.776 0.661 0.004 0.036 0.066 0.123 0.057\nHTML-Tra 0.425 0.522 0.549 0.637 0.533 0.552 0.609 0.671 0.776 0.652 -0.016 0.037 0.021 0.123 0.041\nHTML-Avg 0.465 0.527 0.529 0.564 0.521 0.608 0.629 0.648 0.703 0.647 0.019 0.058 0.019 0.063 0.040\nMRDM 0.504 0.513 0.584 0.619 0.555 0.541 0.663 0.722 0.754 0.670 0.079 0.007 0.107 0.044 0.059\nMRDM-Add 0.416 0.496 0.434 0.496 0.460 0.507 0.655 0.289 0.627 0.520 -0.079 -0.073 -0.073 -0.145 -0.092\nMRDM-Neg 0.619 0.513 0.434 0.381 0.487 0.746 0.667 0.600 0.539 0.638 0.153 0.161 -0.018 0.013 0.077\nMRDM-Sym 0.425 0.531 0.584 0.628 0.542 0.504 0.683 0.697 0.753 0.659 -0.067 0.101 0.111 0.100 0.061\nMRDM-Tra 0.398 0.504 0.575 0.637 0.529 0.521 0.663 0.727 0.776 0.672 -0.105 -0.037 0.108 0.123 0.022\nMRDM-Avg 0.465 0.511 0.507 0.535 0.504 0.569 0.667 0.578 0.674 0.622 -0.024 0.038 0.032 0.023 0.017\nEvent 0.416 0.522 0.593 0.637 0.542 0.582 0.682 0.736 0.776 0.694 0.078 0.097 0.189 0.123 0.122\nEvent-Add 0.425 0.522 0.575 0.637 0.540 0.558 0.671 0.652 0.745 0.657 -0.007 0.044 0.116 0.157 0.077\nEvent-Neg 0.531 0.478 0.381 0.381 0.442 0.686 0.638 0.545 0.539 0.602 -0.151 -0.049 -0.246 0.013 -0.108\nEvent-Sym 0.416 0.504 0.593 0.628 0.535 0.571 0.671 0.726 0.767 0.684 0.003 -0.092 0.138 0.051 0.025\nEvent-Tra 0.416 0.513 0.566 0.628 0.531 0.577 0.675 0.707 0.767 0.681 0.025 0.004 0.042 0.051 0.030\nEvent-Avg 0.447 0.504 0.529 0.569 0.512 0.598 0.663 0.658 0.705 0.656 -0.032 -0.023 0.013 0.068 0.006\nXGB 0.434 0.487 0.584 0.558 0.515 0.448 0.500 0.641 0.653 0.561 -0.093 -0.027 0.147 0.043 0.018\nXGB-Add 0.398 0.504 0.593 0.575 0.518 0.433 0.533 0.657 0.676 0.575 -0.156 0.006 0.160 0.064 0.018\nXGB-Neg 0.549 0.469 0.398 0.451 0.467 0.622 0.444 0.404 0.492 0.490 0.062 -0.064 -0.187 0.011 -0.045\nXGB-Sym 0.434 0.496 0.575 0.566 0.518 0.448 0.513 0.636 0.662 0.565 -0.093 -0.010 0.127 0.058 0.021\nXGB-Tra 0.469 0.540 0.558 0.531 0.524 0.318 0.581 0.638 0.658 0.549 -0.115 0.076 0.078 -0.075 -0.009\nXGB-Avg 0.462 0.502 0.531 0.531 0.507 0.456 0.518 0.584 0.622 0.545 -0.076 0.002 0.045 0.014 -0.004\nTable 5: Ablation study results of four different types of logical transformation based on the fine-tuned forecasting\nmodels. Compared to the original results, the decreased performance is presented in bold .\nStrategy Profit Ratio Sharpe Ratio Transformations Profit Ratio Sharpe Ratio Transformations Profit Ratio Sharpe Ratio\nHTML-Original 3.752 0.266 HTML-ADD 2.282‚Üì 0.125‚Üì HTML-NEG 3.720‚Üì 0.263‚Üì\nHTML+FinTrust 3.359‚Üì 0.229‚Üì HTML-SYM 3.720‚Üì 0.263‚Üì HTML-TRA 3.713‚Üì 0.263‚Üì\nEvent-Original 3.720 0.263 Event-ADD 3.646‚Üì 0.256‚Üì Event-NEG 3.347‚Üì 0.226‚Üì\nEvent+FinTrust 3.535‚Üì 0.245‚Üì Event-SYM 3.494‚Üì 0.241‚Üì Event-TRA 3.652‚Üì 0.256‚Üì\nMRDM-Original 3.495 0.241 MRDM-ADD 0.605‚Üì -0.026‚Üì MRDM-NEG 3.512‚Üë 0.243‚Üë\nMRDM+FinTrust 2.384‚Üì 0.138‚Üì MRDM-SYM 1.674‚Üì 0.070‚Üì MRDM-TRA 3.743‚Üë 0.266‚Üë\nXGB-Original -0.515 -0.126 XGB-ADD 0.972‚Üë 0.006‚Üë XGB-NEG -0.833‚Üì -0.067‚Üë\nXGB+FinTrust 0.296‚Üë -0.032‚Üë XGB-SYM -0.072‚Üë -0.087‚Üë XGB-TRA 1.118‚Üë 0.020‚Üë\nTable 6: The ablation study of the trading simulation based on text-based fine-tuned forecasting models."]